{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 20: 30 Days of python programming\n",
    "### Topic : PIP\n",
    "## Exercise: Level 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read this url and find the 10 most frequent words. romeo_and_juliet = 'http://www.gutenberg.org/files/1112/1112.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " response object: <Response [200]>\n",
      " response status code: 200\n",
      " response headers: {'Date': 'Mon, 27 Feb 2023 19:29:23 GMT', 'Server': 'Apache', 'Last-Modified': 'Sat, 17 Oct 2020 15:05:23 GMT', 'Accept-Ranges': 'bytes', 'Content-Length': '179410', 'X-Backend': 'gutenweb1', 'Content-Type': 'text/plain'}\n",
      " result: [(766, 'the'), (583, 'I'), (555, 'and'), (533, 'to'), (487, 'of'), (460, 'a'), (341, 'in'), (334, 'is'), (318, 'you'), (310, 'my')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "url = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "response = requests.get(url)\n",
    "print(f' response object: {response}')\n",
    "print(f' response status code: {response.status_code}')\n",
    "print(f' response headers: {response.headers}')\n",
    "text = response.text\n",
    "text = re.sub(r'[^\\w\\s]','',text)\n",
    "words = text.split()\n",
    "words_dict = {}\n",
    "for word in words:\n",
    "    words_dict[word] = words_dict.get(word,0) + 1\n",
    "words_sorted = sorted(words_dict.items(),key=lambda x:x[1],reverse=True)\n",
    "result = [(word[1],word[0]) for word in words_sorted]\n",
    "print(f' result: {result[:10]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read the cats API and cats_api = 'https://api.thecatapi.com/v1/breeds' and find :\n",
    "i. the min, max, mean, median, standard deviation of cats' weight in metric units.\n",
    "ii. the min, max, mean, median, standard deviation of cats' lifespan in years.\n",
    "iii. Create a frequency table of country and breed of cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3 - 5', '3 - 5', '3 - 7', '2 - 5', '4 - 7']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "cats_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "\n",
    "response = requests.get(cats_api)\n",
    "\n",
    "if response.status_code == 200:\n",
    "  cat_breeds = response.json()\n",
    "else:\n",
    "  print(\"Failed to retrieve information from API\")\n",
    "cat_weight_metric = []\n",
    "for i in range(len(cat_breeds)):\n",
    "  cat_weight_metric.append(cat_breeds[i]['weight']['metric'])\n",
    "cat_weight_metric[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Read the countries API and find\n",
    "i. the 10 largest countries\n",
    "ii. the 10 most spoken languages\n",
    "iii. the total number of languages in the countries API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser # web browser module to open websites\n",
    "\n",
    "# list of urls: python\n",
    "url_lists = [\n",
    "    'http://www.python.org',\n",
    "    'https://www.linkedin.com/in/asabeneh/',\n",
    "    'https://github.com/Asabeneh',\n",
    "    'https://twitter.com/Asabeneh',\n",
    "]\n",
    "\n",
    "# opens the above list of websites in a different tab\n",
    "for url in url_lists:\n",
    "    webbrowser.open_new_tab(url)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. UCI is one of the most common places to get data sets for data science and machine learning. Read the content of UCL (https://archive.ics.uci.edu/ml/datasets.php). Without additional libraries it will be difficult, so you may try it with BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "\n",
    "# Send a GET request to the URL and retrieve the HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup4\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the table that contains the list of data sets\n",
    "table = soup.find('table', {'border': '1'})\n",
    "\n",
    "# Extract the name of each data set from the table rows, for now, a humble goal due to my current limited skills\n",
    "rows = table.find_all('tr')[1:]\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    name = cells[0].text.strip()\n",
    "    # Gives the names of all the datasets in the table\n",
    "    print(f'{name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2ab3825ac7005fb7b26f112e9c99ae62f464c629e30b0d534c3b931b6cbc3ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
